{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "# NYC Taxi Tip Prediction\n### Deep Learning Regression on 5.6M+ NYC Yellow Taxi Trip Records\n\nThis notebook builds a neural network to predict taxi tip amounts using trip metadata from the [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) (Aug-Sep 2023).\n\n**Pipeline**: Data Loading → Feature Engineering → Outlier Removal → Encoding & Scaling → Neural Network Training → Evaluation & Visualization\n\n<a href=\"https://colab.research.google.com/github/ninadpshah/New-York-TLC-Tip-Prediction/blob/main/TLC_Trip_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Environment Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Mount Google Drive (Colab only) to access the raw trip data files\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Core libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nimport tensorflow as tf\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU available: {bool(tf.config.list_physical_devices('GPU'))}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9wr0wF0eQh7",
    "outputId": "bfb8bf82-7520-40d0-af15-5e6c8484a0f5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Data Loading\n\nLoad NYC TLC Yellow Taxi trip records for **August and September 2023** (~6M+ trips). Source: [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load August and September 2023 yellow taxi trip data\nDATA_DIR = '/content/drive/MyDrive/TLC_Yellow/'\ndate_cols = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n\naugust_data = pd.read_csv(\n    f'{DATA_DIR}yellow_tripdata_2023-08.csv',\n    parse_dates=date_cols, low_memory=False\n)\nseptember_data = pd.read_csv(\n    f'{DATA_DIR}yellow_tripdata_2023-09.csv',\n    parse_dates=date_cols, low_memory=False\n)\n\ncombined_data = pd.concat([august_data, september_data], ignore_index=True)\nprint(f\"Combined dataset: {len(combined_data):,} trips\")\nprint(f\"Columns: {list(combined_data.columns)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Data Preprocessing\n\nFeature engineering, outlier removal, encoding, and train/test split.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- Feature Selection ---\nselected_features = ['trip_distance', 'RatecodeID', 'fare_amount',\n                     'payment_type', 'Airport_fee', 'tip_amount']\ntaxi_filtered = combined_data[selected_features].copy()\n\n# --- Temporal Feature Engineering ---\n# Extract time-based features to capture commuter patterns and time-of-day effects\nfor prefix, col in [('pickup', 'tpep_pickup_datetime'),\n                     ('dropoff', 'tpep_dropoff_datetime')]:\n    dt = combined_data[col]\n    taxi_filtered[f'{prefix}_weekday'] = dt.dt.weekday\n    taxi_filtered[f'{prefix}_hour'] = dt.dt.hour\n    taxi_filtered[f'{prefix}_minute'] = dt.dt.minute\n    taxi_filtered[f'{prefix}_week_hour'] = (\n        taxi_filtered[f'{prefix}_weekday'] * 24 + taxi_filtered[f'{prefix}_hour']\n    )\n\nprint(f\"Features after engineering: {list(taxi_filtered.columns)}\")\nprint(f\"Shape: {taxi_filtered.shape}\")\n\n# --- Outlier Removal (Z-score) ---\ndef remove_outliers_zscore(df, columns, threshold=3):\n    \"\"\"Remove rows where any specified column has |z-score| >= threshold.\"\"\"\n    z_scores = np.abs(stats.zscore(df[columns]))\n    return df[(z_scores < threshold).all(axis=1)]\n\ncolumns_to_check = ['trip_distance', 'fare_amount']\ntaxi_filtered_no_outliers = remove_outliers_zscore(taxi_filtered, columns_to_check)\nprint(f\"After outlier removal: {len(taxi_filtered_no_outliers):,} records \"\n      f\"(removed {len(taxi_filtered) - len(taxi_filtered_no_outliers):,})\")\n\n# --- Categorical Encoding ---\nlabel_encoders = {}\ncategorical_columns = ['RatecodeID', 'payment_type', 'Airport_fee']\nfor col in categorical_columns:\n    label_encoders[col] = LabelEncoder()\n    taxi_filtered_no_outliers[col] = label_encoders[col].fit_transform(\n        taxi_filtered_no_outliers[col]\n    )\n\n# --- Train/Test Split & Scaling ---\nX = taxi_filtered_no_outliers.drop('tip_amount', axis=1)\ny = taxi_filtered_no_outliers['tip_amount']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"\\nTrain set: {X_train_scaled.shape[0]:,} samples\")\nprint(f\"Test set:  {X_test_scaled.shape[0]:,} samples\")\nprint(f\"Features:  {X_train_scaled.shape[1]}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Exploratory Data Analysis\n\nInspect the cleaned dataset and visualize distributions, correlations, and feature relationships.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Dataset summary after preprocessing\nprint(f\"Dataset shape: {taxi_filtered_no_outliers.shape}\")\nprint(f\"Missing values: {taxi_filtered_no_outliers.isnull().sum().sum()}\")\nprint(f\"\\n{'='*60}\")\nprint(\"Feature Statistics:\")\nprint(f\"{'='*60}\")\ntaxi_filtered_no_outliers.describe().round(2)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FUzdn0uj51Y",
    "outputId": "473b7905-39a2-4776-97bd-a2ec4790575d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Target variable distribution\nplt.figure(figsize=(10, 5))\nsns.histplot(taxi_filtered_no_outliers['tip_amount'], bins=50, kde=True, color='steelblue')\nplt.xlabel('Tip Amount ($)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Tip Amount (5.6M+ trips)')\nplt.axvline(taxi_filtered_no_outliers['tip_amount'].mean(), color='red',\n            linestyle='--', label=f\"Mean: ${taxi_filtered_no_outliers['tip_amount'].mean():.2f}\")\nplt.legend()\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "id": "daPwhol_5Dcd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "outputId": "966540e5-f748-42f6-b4b5-c9eee9b03afc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Feature correlation heatmap\nplt.figure(figsize=(12, 9))\ncorr = taxi_filtered_no_outliers.select_dtypes(include=[np.number]).corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Feature Correlation Heatmap')\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59vUO_go8apu",
    "outputId": "2624a8e9-370d-49a7-aa4b-677f196eb913"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Pairwise feature relationships (sampled for performance)\nsns.pairplot(\n    taxi_filtered_no_outliers.sample(5000, random_state=42),\n    vars=['trip_distance', 'fare_amount', 'pickup_weekday', 'pickup_hour', 'tip_amount'],\n    diag_kind='kde',\n    plot_kws={'alpha': 0.4, 's': 10}\n)\nplt.suptitle('Pairplot of Selected Features', y=1.02)\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "w1htoqnZ8bBR",
    "outputId": "c9962da3-deb4-4c10-ca59-9c79fc76606a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Model Training\n\nBuild a feed-forward neural network with dropout regularization for the regression task:\n- **Architecture**: Dense(128, ReLU) → Dropout(0.5) → Dense(64, ReLU) → Dense(1)\n- **Optimizer**: Adam (lr=0.001)\n- **Loss**: Mean Squared Error\n- **Training**: 50 epochs, batch size 8,196, 20% validation split, GPU-accelerated",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "pQmDzaY58b95",
    "outputId": "3cf1d9a3-f7fa-4d44-c840-a7efa9fd969a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Build the model\nmodel = Sequential([\n    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dense(1)  # Linear output for regression\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='mean_squared_error'\n)\n\nmodel.summary()\n\n# Train with GPU acceleration\ndevice = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\nprint(f\"\\nTraining on: {device}\")\n\nwith tf.device(device):\n    history = model.fit(\n        X_train_scaled, y_train,\n        epochs=50,\n        batch_size=8196,\n        validation_split=0.2,\n        verbose=1\n    )\n\n# Evaluate on held-out test set\ntest_loss = model.evaluate(X_test_scaled, y_test)\npredictions = model.predict(X_test_scaled)\n\n# Compute additional metrics\ntest_rmse = np.sqrt(test_loss)\ntest_r2 = r2_score(y_test, predictions)\ntest_mae = np.mean(np.abs(y_test.values - predictions.flatten()))\n\nprint(f\"\\n{'='*40}\")\nprint(f\"Test MSE:  {test_loss:.4f}\")\nprint(f\"Test RMSE: ${test_rmse:.2f}\")\nprint(f\"Test MAE:  ${test_mae:.2f}\")\nprint(f\"Test R²:   {test_r2:.4f}\")\nprint(f\"{'='*40}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HGMLVlmy8c57",
    "outputId": "dab7292e-1697-40d7-c90b-f0b736501e4c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Evaluation & Visualization\n\nVisualize training convergence, prediction distributions, and compare true vs. predicted tip amounts across key features.",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "id": "c5331MNQBX_d",
    "outputId": "a10d0afb-1054-4ac7-966e-f9e3947a3083"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# --- Training Convergence ---\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(history.history['loss'], label='Training Loss', linewidth=2)\nax.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\nax.set_xlabel('Epoch')\nax.set_ylabel('MSE Loss')\nax.set_title('Training & Validation Loss over Epochs')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# --- Prediction Density (KDE) ---\npredictions_df = pd.DataFrame({\n    'True Values': y_test.values,\n    'Predictions': predictions.flatten()\n})\nsample = predictions_df.sample(9999, random_state=42)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\nfor ax in axs:\n    ax.set(xlim=[-3, 20])\nsns.kdeplot(data=sample, ax=axs[0], bw_adjust=3)\naxs[0].set_title('Prediction Density (KDE)')\naxs[0].set_xlabel('Tip Amount ($)')\nsns.kdeplot(data=sample, ax=axs[1], bw_adjust=3, cumulative=True)\naxs[1].set_title('Cumulative Prediction Density')\naxs[1].set_xlabel('Tip Amount ($)')\nfig.tight_layout()\nplt.show()\n\n# --- True vs Predicted: Correlation Heatmap ---\nselected_features_without_tip = [f for f in selected_features if f != 'tip_amount']\ncomparison_df = pd.DataFrame({\n    'True Tip Amount': y_test.values,\n    'Predicted Tip Amount': predictions.flatten()\n})\ncomparison_df[selected_features_without_tip] = X_test.reset_index(drop=True)[selected_features_without_tip]\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(comparison_df.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation: Features vs True & Predicted Tip Amounts')\nplt.tight_layout()\nplt.show()\n\n# --- True vs Predicted: Scatter Plots ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nfor ax, feature in zip(axes, ['trip_distance', 'fare_amount']):\n    ax.scatter(X_test[feature], y_test, alpha=0.15, s=3, label='True', color='steelblue')\n    ax.scatter(X_test[feature], predictions.flatten(), alpha=0.15, s=3, label='Predicted', color='coral')\n    ax.set_xlabel(feature.replace('_', ' ').title())\n    ax.set_ylabel('Tip Amount ($)')\n    ax.set_title(f'{feature.replace(\"_\", \" \").title()} vs Tip Amount')\n    ax.legend()\nfig.tight_layout()\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "gjwzAnLBDcfS",
    "outputId": "2abc222d-c95e-4e73-d207-3ee6e2b66794"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}